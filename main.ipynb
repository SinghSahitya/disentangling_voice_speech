{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# VAE class shared by all layers\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2 * latent_dim)  # Mean and log-variance\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()  # To reconstruct the input features\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        mu_logvar = self.encoder(x).view(-1, 2, self.latent_dim)\n",
    "        mu = mu_logvar[:, 0, :]  # Mean\n",
    "        logvar = mu_logvar[:, 1, :]  # Log-variance\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decoder\n",
    "        reconstructed_x = self.decoder(z)\n",
    "        return reconstructed_x, mu, logvar\n",
    "\n",
    "# Layer 1: Precursor Speaker Representation\n",
    "class PrecursorSpeakerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(PrecursorSpeakerLayer, self).__init__()\n",
    "        self.vae = VAE(input_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Use VAE to extract precursor speaker embedding\n",
    "        reconstructed_x, mu, logvar = self.vae(x)\n",
    "        return reconstructed_x, mu, logvar\n",
    "\n",
    "# Layer 2: Disentangled Content Representation\n",
    "class DisentangledContentLayer(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(DisentangledContentLayer, self).__init__()\n",
    "        self.vae = VAE(input_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x, speaker_mu):\n",
    "        # Subtract speaker_mu from x to disentangle content\n",
    "        content_input = x - speaker_mu\n",
    "        \n",
    "        reconstructed_x, content_mu, content_logvar = self.vae(content_input)\n",
    "        return reconstructed_x, content_mu, content_logvar\n",
    "\n",
    "# Layer 3: Final Disentangled Speaker Representation\n",
    "class FinalSpeakerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(FinalSpeakerLayer, self).__init__()\n",
    "        self.vae = VAE(input_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x, content_mu):\n",
    "        # Subtract content_mu from x to get final speaker embedding\n",
    "        speaker_input = x - content_mu\n",
    "        \n",
    "        reconstructed_x, final_speaker_mu, final_speaker_logvar = self.vae(speaker_input)\n",
    "        return reconstructed_x, final_speaker_mu, final_speaker_logvar\n",
    "\n",
    "# Full Temporal Aggregation Module (Layer 1 -> Layer 2 -> Layer 3)\n",
    "class TemporalAggregation(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(TemporalAggregation, self).__init__()\n",
    "        self.layer1 = PrecursorSpeakerLayer(input_dim, latent_dim)\n",
    "        self.layer2 = DisentangledContentLayer(input_dim, latent_dim)\n",
    "        self.layer3 = FinalSpeakerLayer(input_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: Precursor Speaker Representation\n",
    "        precursor_output, speaker_mu, speaker_logvar = self.layer1(x)\n",
    "        \n",
    "        # Layer 2: Disentangled Content Representation\n",
    "        content_output, content_mu, content_logvar = self.layer2(x, speaker_mu)\n",
    "        \n",
    "        # Layer 3: Final Disentangled Speaker Representation\n",
    "        final_speaker_output, final_speaker_mu, final_speaker_logvar = self.layer3(x, content_mu)\n",
    "        \n",
    "        return {\n",
    "            'precursor_output': precursor_output,\n",
    "            'speaker_mu': speaker_mu,\n",
    "            'speaker_logvar': speaker_logvar,\n",
    "            'content_output': content_output,\n",
    "            'content_mu': content_mu,\n",
    "            'content_logvar': content_logvar,\n",
    "            'final_speaker_output': final_speaker_output,\n",
    "            'final_speaker_mu': final_speaker_mu,\n",
    "            'final_speaker_logvar': final_speaker_logvar\n",
    "        }\n",
    "\n",
    "# Example of running the complete Temporal Aggregation module\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_dim = 40  # Example: MFCC features with 40 dimensions\n",
    "    latent_dim = 16  # Latent space dimension\n",
    "    \n",
    "    # Simulated encoder output (batch of MFCCs)\n",
    "    batch_size = 32\n",
    "    sample_input = torch.randn(batch_size, input_dim)  # Batch of audio feature vectors (MFCCs)\n",
    "    \n",
    "    # Instantiate the Temporal Aggregation module\n",
    "    temporal_aggregation = TemporalAggregation(input_dim, latent_dim)\n",
    "    \n",
    "    # Pass input through all three layers (Layer 1 -> Layer 2 -> Layer 3)\n",
    "    output = temporal_aggregation(sample_input)\n",
    "    \n",
    "    # Print output shapes for each layer\n",
    "    print(\"Precursor Output Shape:\", output['precursor_output'].shape)\n",
    "    print(\"Speaker Mu Shape:\", output['speaker_mu'].shape)\n",
    "    print(\"Content Output Shape:\", output['content_output'].shape)\n",
    "    print(\"Content Mu Shape:\", output['content_mu'].shape)\n",
    "    print(\"Final Speaker Output Shape:\", output['final_speaker_output'].shape)\n",
    "    print(\"Final Speaker Mu Shape:\", output['final_speaker_mu'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
