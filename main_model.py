# -*- coding: utf-8 -*-
"""main_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uf8fMWLiXZ0RN1Le1ezcwKerEnOWo82_
"""

import torch
from torch import nn
from torch.utils.data import DataLoader,Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
from tqdm import tqdm
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

class MELDDataset(Dataset):
    def __init__(self, pkl_file, max_len=500, speaker_mapping=None):
        with open(pkl_file, 'rb') as f:
            self.data = pickle.load(f)
        self.max_len = max_len
        self.speaker_mapping = speaker_mapping

    def __getitem__(self, idx):
        sample = self.data[idx]
        features = torch.tensor(sample['features'], dtype=torch.float32)
        # Truncate and normalize features
        features = features[:, :self.max_len]
        features = (features - features.mean()) / (features.std() + 1e-6)
        speaker_label = self.speaker_mapping[sample['speaker']]
        emotion_label = sample['emotion']
        emotion_to_index = {'neutral': 0, 'joy': 1, 'sadness': 2, 'anger': 3, 'surprise': 4, 'fear': 5, 'disgust': 6}
        emotion_index = emotion_to_index[emotion_label]
        return features.T, speaker_label, emotion_index  # Return three separate elements

    def __len__(self):
        return len(self.data)

import pickle

def get_num_speakers(pkl_file):
    """
    Calculate the number of unique speakers in the dataset.
    :param pkl_file: Path to the .pkl file.
    :return: Number of unique speakers.
    """
    with open(pkl_file, 'rb') as f:
        data = pickle.load(f)
    speakers = [sample['speaker'] for sample in data]
    unique_speakers = set(speakers)
    return len(unique_speakers), {speaker: idx for idx, speaker in enumerate(unique_speakers)}

from torch.nn.utils.rnn import pad_sequence
def custom_collate_fn(batch):
    features = [item[0] for item in batch]
    speaker_labels = [item[1] for item in batch]
    emotion_labels = [item[2] for item in batch]
    features_padded = pad_sequence(features, batch_first=True, padding_value=0.0)  # Pad sequences
    speaker_labels = torch.tensor(speaker_labels, dtype=torch.long)
    emotion_labels = torch.tensor(emotion_labels, dtype=torch.long)
    return features_padded, speaker_labels, emotion_labels

def prepare_dataloaders(pkl_path, batch_size=32, shuffle=True):
    dataset = MELDDataset(pkl_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)
    return dataloader

import torch.nn.utils.rnn as rnn_utils
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(Encoder, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim)
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),  # Ensure consistent output dimension
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, x):
        x = x.permute(0, 2, 1)  # [batch_size, seq_len, feature_dim] -> [batch_size, feature_dim, seq_len]
        conv_out = self.conv_layers(x)
        conv_out = torch.mean(conv_out, dim=2)  # Pooling over the sequence dimension
        frame_level_features = self.fc(conv_out)
        return frame_level_features  # Output shape: [batch_size, hidden_dim]



class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        self.latent_dim = latent_dim

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 2 * latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Sigmoid()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        # Encoder
        mu_logvar = self.encoder(x).view(-1, 2, self.latent_dim)
        mu = mu_logvar[:, 0, :]
        logvar = mu_logvar[:, 1, :]

        z = self.reparameterize(mu, logvar)

        reconstructed_x = self.decoder(z)
        return reconstructed_x, mu, logvar


class PrecursorSpeakerLayer(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(PrecursorSpeakerLayer, self).__init__()
        self.vae = VAE(input_dim, latent_dim)

    def forward(self, x):
        reconstructed_x, mu, logvar = self.vae(x)
        return reconstructed_x, mu, logvar

class DisentangledContentLayer(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(DisentangledContentLayer, self).__init__()
        self.vae = VAE(latent_dim, latent_dim)
        self.project_to_latent = nn.Linear(input_dim, latent_dim)

    def forward(self, x, speaker_mu):
        projected_x = self.project_to_latent(x)
        content_input = projected_x - speaker_mu
        reconstructed_x, content_mu, content_logvar = self.vae(content_input)
        return reconstructed_x, content_mu, content_logvar

class FinalSpeakerLayer(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(FinalSpeakerLayer, self).__init__()
        self.vae = VAE(latent_dim, latent_dim)
        self.project_to_latent = nn.Linear(input_dim, latent_dim)

    def forward(self, x, content_mu):
        projected_x = self.project_to_latent(x)
        speaker_input = projected_x - content_mu
        reconstructed_x, final_speaker_mu, final_speaker_logvar = self.vae(speaker_input)
        return reconstructed_x, final_speaker_mu, final_speaker_logvar

class TemporalAggregation(nn.Module):
    def __init__(self, hidden_dim, latent_dim):
        super(TemporalAggregation, self).__init__()
        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim
        self.precursor_layer = nn.Linear(hidden_dim, latent_dim)
        self.content_layer = nn.Linear(hidden_dim, latent_dim)
        self.speaker_layer = nn.Linear(hidden_dim, latent_dim)

    def forward(self, encoded_features):
        """
        Encodes the precursor speaker, content, and disentangled speaker representations.
        :param encoded_features: Output of the Encoder (batch_size, hidden_dim)
        :return: precursor_speaker, content, disentangled_speaker
        """
        precursor = self.precursor_layer(encoded_features)  # Ensure dimensions match
        content = self.content_layer(encoded_features)
        disentangled_speaker = self.speaker_layer(encoded_features)
        return precursor, content, disentangled_speaker



class EmotionRecognitionBranch(nn.Module):
    def __init__(self, input_dim, num_emotions):
        super(EmotionRecognitionBranch, self).__init__()
        self.fc_layers = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, num_emotions)
        )

    def forward(self, x):
        emotion_logits = self.fc_layers(x)
        return emotion_logits

class EmotionAdversarialDiscriminator(nn.Module):
    def __init__(self, latent_dim, num_emotions):
        super(EmotionAdversarialDiscriminator, self).__init__()
        self.fc_layers = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_emotions)
        )

    def forward(self, speaker_embedding):
        emotion_logits = self.fc_layers(speaker_embedding)
        return emotion_logits

class Decoder(nn.Module):
    def __init__(self, latent_dim, output_dim):
        super(Decoder, self).__init__()
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim),
            nn.Sigmoid()
        )

    def forward(self, speaker_embedding):
        reconstructed_input = self.decoder(speaker_embedding)
        return reconstructed_input
class FullModelWithEncoder(nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dim, num_emotions):
        super(FullModelWithEncoder, self).__init__()
        self.encoder = Encoder(input_dim, hidden_dim)
        self.temporal_aggregation = TemporalAggregation(hidden_dim, latent_dim)
        self.emotion_recognition = EmotionRecognitionBranch(latent_dim, num_emotions)
        self.speaker_classifier = nn.Linear(latent_dim, num_speakers)  # Latent_dim -> num_speakers

    def forward(self, x, lengths=None):
        encoded_features = self.encoder(x)  # Output: [batch_size, hidden_dim]
        precursor_speaker, content, disentangled_speaker = self.temporal_aggregation(encoded_features)
        emotion_logits = self.emotion_recognition(disentangled_speaker)  # Input: disentangled_speaker
        speaker_logits = self.speaker_classifier(disentangled_speaker)  # Input: disentangled_speaker
        return speaker_logits, emotion_logits, precursor_speaker, content, disentangled_speaker




def build_model(input_dim=13, latent_dim=16, hidden_dim=64, num_emotions=6):
    return FullModelWithEncoder(input_dim, latent_dim, hidden_dim, num_emotions)

def self_supervision_loss(precursor, content, disentangled_speaker):
    """
    Calculate the self-supervision speaker preserving loss (Lssp).
    """
    speaker_representation = precursor - content
    loss = nn.MSELoss()(speaker_representation, disentangled_speaker)
    return loss

from collections import Counter

def calculate_class_weights(labels):
    class_counts = Counter(labels)
    total_samples = sum(class_counts.values())
    num_classes = len(class_counts)
    weights = [total_samples / (num_classes * class_counts[i]) for i in range(num_classes)]
    return torch.tensor(weights, dtype=torch.float32)

def train_model_with_meld(model, train_loader, num_speakers, num_emotions, num_epochs=50, lr=0.0001, save_path='best_model.pth'):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    speaker_loss_fn = nn.CrossEntropyLoss()
    emotion_loss_fn = nn.CrossEntropyLoss()

    alpha, beta, gamma = 1.0, 0.5, 0.2  # Loss weighting hyperparameters

    for epoch in range(num_epochs):
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        model.train()
        total_speaker_loss, total_emotion_loss, total_ssp_loss = 0, 0, 0
        all_speaker_preds, all_emotion_preds, all_labels = [], [], []

        train_loader_tqdm = tqdm(train_loader, desc=f"Training Epoch {epoch+1}/{num_epochs}", leave=False)

        for features_batch, speaker_labels, emotion_labels in train_loader_tqdm:
            features_batch = features_batch.to(device)
            speaker_labels = speaker_labels.to(device)
            emotion_labels = emotion_labels.to(device)

            optimizer.zero_grad()

            # Forward pass
            speaker_logits, emotion_logits, precursor, content, disentangled_speaker = model(features_batch, None)

            # Speaker classification loss
            speaker_loss = speaker_loss_fn(speaker_logits, speaker_labels)

            # Emotion classification loss
            emotion_loss = emotion_loss_fn(emotion_logits, emotion_labels)

            # Self-supervision loss
            ssp_loss = self_supervision_loss(precursor, content, disentangled_speaker)

            # Combined loss
            total_loss = alpha * speaker_loss + beta * ssp_loss + gamma * emotion_loss
            total_loss.backward()

            optimizer.step()

            # Track losses
            total_speaker_loss += speaker_loss.item()
            total_emotion_loss += emotion_loss.item()
            total_ssp_loss += ssp_loss.item()

            # Metrics
            speaker_preds = torch.argmax(speaker_logits, dim=1).cpu().numpy()
            emotion_preds = torch.argmax(emotion_logits, dim=1).cpu().numpy()

            all_speaker_preds.extend(speaker_preds)
            all_emotion_preds.extend(emotion_preds)
            all_labels.extend(speaker_labels.cpu().numpy())

        speaker_accuracy = accuracy_score(all_labels, all_speaker_preds)
        print(f"Epoch {epoch+1}, Speaker Loss: {total_speaker_loss:.4f}, Emotion Loss: {total_emotion_loss:.4f}, SSP Loss: {total_ssp_loss:.4f}, Speaker Accuracy: {speaker_accuracy:.4f}")

        # Save the model
        torch.save(model.state_dict(), save_path)

if __name__ == "__main__":
    # Paths
    pkl_path = "/content/drive/MyDrive/op.pkl"
    save_model_path = "/content/drive/MyDrive/best_model.pth"

    # Hyperparameters
    input_dim = 13  # MFCC features
    latent_dim = 16
    hidden_dim = 64
    num_emotions = 7  # Number of emotion classes
    batch_size = 32
    num_epochs = 500
    learning_rate = 0.0001

    # Calculate the number of speakers
    num_speakers, speaker_mapping = get_num_speakers(pkl_path)

    # Load dataset and dataloader
    train_loader = DataLoader(
        dataset=MELDDataset(pkl_path, speaker_mapping=speaker_mapping),
        batch_size=batch_size,
        shuffle=True,
        collate_fn=custom_collate_fn
    )

    # Build and train the model
    model = FullModelWithEncoder(input_dim=input_dim, latent_dim=latent_dim, hidden_dim=hidden_dim, num_emotions=num_emotions)
    train_model_with_meld(model, train_loader, num_speakers, num_emotions, num_epochs=num_epochs, lr=learning_rate, save_path=save_model_path)

from sklearn.metrics import roc_curve
import numpy as np
import torch.nn.functional as F


def cosine_similarity(a, b):
    """
    Compute cosine similarity between two tensors.
    :param a: Tensor of embeddings [batch_size, embedding_dim]
    :param b: Tensor of embeddings [batch_size, embedding_dim]
    :return: Cosine similarity scores [batch_size]
    """
    a_norm = F.normalize(a, p=2, dim=1)
    b_norm = F.normalize(b, p=2, dim=1)
    return torch.sum(a_norm * b_norm, dim=1)

def compute_eer(scores, labels):
    """
    Compute Equal Error Rate (EER).
    :param scores: Similarity scores
    :param labels: Ground truth labels (1 for same speaker, 0 for different speakers)
    :return: EER value
    """
    fpr, tpr, thresholds = roc_curve(labels, scores)
    fnr = 1 - tpr
    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]
    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]
    return eer, eer_threshold

def compute_min_dcf(scores, labels, p_target=0.01, c_miss=1, c_fa=1):
    """
    Compute Minimum Detection Cost Function (minDCF).
    :param scores: Similarity scores
    :param labels: Ground truth labels
    :param p_target: Prior probability of target speaker
    :param c_miss: Cost of a miss
    :param c_fa: Cost of a false acceptance
    :return: minDCF value
    """
    fpr, tpr, thresholds = roc_curve(labels, scores)
    fnr = 1 - tpr
    min_dcf = float('inf')

    for threshold in thresholds:
        miss = fnr[np.where(thresholds == threshold)[0][0]]
        fa = fpr[np.where(thresholds == threshold)[0][0]]
        dcf = p_target * c_miss * miss + (1 - p_target) * c_fa * fa
        min_dcf = min(min_dcf, dcf)

    return min_dcf
def evaluate_model_with_meld(model, train_loader, device):
    model.eval()
    all_scores, all_labels = [], []

    with torch.no_grad():
        for features, speaker_labels, _ in tqdm(train_loader, desc="Evaluating", leave=False):
            features, speaker_labels = features.to(device), speaker_labels.to(device)

            # Forward pass
            speaker_logits, _, _, _, disentangled_speaker = model(features)

            # Compute pairwise scores and binary labels
            for i in range(disentangled_speaker.size(0)):
                for j in range(i + 1, disentangled_speaker.size(0)):
                    score = cosine_similarity(
                        disentangled_speaker[i].unsqueeze(0), disentangled_speaker[j].unsqueeze(0)
                    ).item()
                    label = 1 if speaker_labels[i] == speaker_labels[j] else 0
                    all_scores.append(score)
                    all_labels.append(label)

    # Compute EER
    eer, eer_threshold = compute_eer(np.array(all_scores), np.array(all_labels))

    # Compute minDCF
    min_dcf = compute_min_dcf(np.array(all_scores), np.array(all_labels))

    return eer, min_dcf
device = "cuda"
print("Evaluating Model...")
eer, min_dcf = evaluate_model_with_meld(model, train_loader, device)
print(f"Final Evaluation Results: EER = {eer:.4f}, minDCF = {min_dcf:.4f}")